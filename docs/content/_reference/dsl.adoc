== The rule DSL

The rule of a production is specified using a DSL built on top of C++ operator overloading.
Everything of the DSL is defined in the namespace `lexy::dsl::*` and every header available under `lexy/dsl/*`.
The umbrella header `lexy/dsl.hpp` includes all DSL headers.

A `Rule` is an object that defines a specific set of input to be parsed.
It first tries to match a set of characters from the input by comparing the character at the current reader position to the set of expected characters,
temporarily advancing the reader further if necessary.
If the matching was successful, a subset of matched characters are consumed by advancing the reader permanently.
The rule can then produce zero or more values, which are eventually forwarded to the value callback of its production.
If the matching was not successful, an error is produced instead.
A failed rule does not consume any characters.

A `Branch` is a rule that has an associated condition.
The parsing algorithm can efficiently check whether the condition would match at the current reader position.
As such, they are used whenever the algorithm needs to decide between multiple alternatives.
Once the branch condition matches, the branch is taken without any additional backtracking.

A `Token` is a special `Rule` that is an atomic element of the input.
As a rule, it does not produce any value.
Every `Token` is also a `Branch` that uses itself as the condition.

=== Whitespace

By default, `lexy` does not treat whitespace in any special way.
You need to instruct it to do so, using either manual or automatic whitespace skipping.

Manual whitespace skipping is done using `lexy::dsl::whitespace(rule)`.
It skips zero or more whitespace characters defined by `rule`.
Insert it everywhere you want to skip over whitespace.
See `examples/email.cpp` or `examples/xml.cpp` for an example of manual whitespace skipping.

Automatic whitespace skipping is done by adding a `static constexpr auto whitespace` to the root production,
i.e. the production passed to one of the parse functions.
This member is initialized to a rule that defines a single whitespace character.
`lexy` will then skip zero or more occurrences of `::whitespace` after every token of the entire grammar.

To temporarily disable whitespace skipping for a production, inherit the production from `lexy::token_production`.
Then whitespace will not be skipped for the rule of the production, and all productions reached from that rule.
Likewise, `lexy::dsl::no_whitespace()` can be used to disable it for a single rule.

See `examples/tutorial.cpp` or `examples/json.cpp` for an example of automatic whitespace skipping.

NOTE: "Whitespace" can mean literal whitespace characters, but also comments (or whatever you want it to mean).

[discrete]
==== `lexy::dsl::whitespace` (explicit)

.`lexy/dsl/whitespace.hpp`
----
whitespace(rule) : Rule

whitespace(rule_a) | rule_b = whitespace(rule_a | rule_b)
whitespace(rule_a) / rule_b = whitespace(rule_a / rule_b)
----

The explicit `whitespace` rule matches `rule` zero or more times and treats the result as whitespace.
This happens regardless of the state of automatic whitespace skipping.

If the whitespace rule is used inside a choice or alternative, the entire choice/alternative is treated as whitespace instead.

Requires::
  `rule` is a branch or a choice rule.
  It must not produce any values.
Matches::
  While the branch condition of `rule` or any of the branch conditions of the choices match,
  match and consume `rule`.
  This will only stop once the branch conditions no longer match.
  While matching and consuming `rule`, automatic whitespace skipping is disabled.
Values::
  None.
Errors::
  All errors raised by `rule` after the branch condition has been matched.

[discrete]
==== `lexy::dsl::whitespace` (implicit)

.`lexy/dsl/whitespace.hpp`
----
whitespace : Rule = whitespace(automatic_whitespace_rule)
----

The implicit `whitespace` rule is equivalent to the explicit `whitespace` rule with the current whitespace rule;
i.e. it matches the current whitespace rule zero or more times.

The current whitespace rule is determined as follows:

* If automatic whitespace skipping is disabled, there is no current whitespace rule.
  `lexy::dsl::whitespace` does nothing.
* If the current production inherits from `lexy::token_production`, there is no current whitespace rule.
  `lexy::dsl::whitespace` does nothing.
* Otherwise, if the current production defines a `static constexpr auto whitespace` member,
  its value is the current whitespace rule.
* Otherwise, if the root production defines a `static constexpr auto whitespace` member,
  its value is the current whitespace rule.

Here, the root production is defined as follows:

* If the current production is a token production, the root production is the current production.
* Otherwise, if the current production is the production that was originally parsed to the top-level parse function (e.g. `lexy::parse()`),
  the root production is the current production.
* Otherwise, the root production is taken from the production that parsed the `lexy::dsl::p` or `lexy::dsl::recurse` rule to start parsing the current production.

This rule is automatically parsed after every token, after a production that inherits from `lexy::token_production`,
or after a `lexy::dsl::no_whitespace()` rule.

.Example
[%collapsible]
====

[source,cpp]
----
struct token_p : lexy::token_production
{
    struct child
    {
        static constexpr auto rule = dsl::whitespace; // <4>
    };

    static constexpr auto rule = dsl::whitespace + dsl::p<child>; // <3>
};

struct normal_prod
{
    static constexpr auto rule = dsl::whitespace + dsl::p<token_p>; // <2>
};

struct root_prod
{
    static constexpr auto whitespace = dsl::ascii::space;
    static constexpr auto rule = dsl::whitespace + dsl::p<normal_prod>; // <1>
};

…

auto result = lexy::parse<root_prod>(…);
----
<1> Here, the automatic whitespace rule is `dsl::ascii::space`,
    as the current production has a `whitespace` member.
<2> Here, the automatic whitespace rule is also `dsl::ascii::space`.
    The current production doesn't have a `whitespace` member,
    but its root production (`root_prod`) does.
<3> Here, the current production is a token production, so there is no automatic whitespace.
    The root production is reset to `token_p`.
<4> Here, the root production is `token_p`, as that is the root of the parent.
    As such, there is no automatic whitespace.
====

[discrete]
==== `lexy::dsl::no_whitespace()`

.`lexy/dsl/whitespace.hpp`
----
no_whitespace(rule)   : Rule
no_whitespace(branch) : Branch
----

The `no_whitespace` rule parses the given `rule` but disables automatic whitespace skipping while doing so.
It is a branch if given a branch.

Branch Condition::
  Whatever `branch` uses as branch condition.
  Note that automatic whitespace skipping inside a branch condition is impossible anyway,
  so nothing is changed there.
Matches::
  Matches and consumes `rule` but without performing automatic whitespace skipping after every token;
  it disables the automatic whitespace rule during the parsing of `rule`.
  After `rule` has been matched, skips implicit whitespace by matching and consuming `lexy::dsl::whitespace`.
Values::
  All values produced by `rule`.
Errors::
  All errors raised by `rule`.

CAUTION: When `rule` contains a `lexy::dsl::p` or `lexy::dsl::recurse` rule, whitespace skipping is re-enabled while that production is parsed.

=== Primitive Tokens

NOTE: All tokens, not just the tokens defined here, do implicit whitespace skipping.
As such, a token `t` is really equivalent to `t + dsl::whitespace`.
This has no effect, unless a whitespace rule has been specified.

[discrete]
==== `lexy::dsl::any`

.`lexy/dsl/any.hpp`
----
any : Token
----

The `any` token matches anything, i.e. all the remaining input.

Matches::
  All the remaining input.
Error::
  n/a (it never fails)

NOTE: `any` is useful in combination with partial inputs such as the minus rule or `switch_`.

[discrete]
==== `lexy::dsl::lit`

.`lexy/dsl/literal.hpp`
----
lit_c<C> : Token
lit<Str> : Token

LEXY_LIT(Str) : Token
----

The literal tokens match the specified sequence of characters.

Requires::
  * `C` is a character literal.
  * `Str` is a string literal.
+
In both cases, their encoding must be ASCII or match the encoding of the input.

Matches::
  The specified character or string of characters, which are consumed.
Error::
  `lexy::expected_literal` giving it the string and the index where the match failure occurred.

WARNING: Don't use a literal token to parse a keyword; use `dsl::keyword()` instead.
Something like `LEXY_LIT("int")` would consume the prefix of `integer`, whereas `dsl::keyword()` does not.

NOTE: `lit<Str>` requires C++20 support for extended NTTPs.
Use the `LEXY_LIT(Str)` macro if your compiler does not support them.

.`lexy/dsl/punctuator.hpp`
----
period    : Token = lit<".">
comma     : Token = lit<",">
colon     : Token = lit<":">
semicolon : Token = lit<";">

hyphen     : Token = lit<"-">
slash      : Token = lit<"/">
backslash  : Token = lit<"\\">
apostrophe : Token = lit<"'">

hash_sign   : Token = lit<"#">
dollar_sign : Token = lit<"$">
at_sign     : Token = lit<"@">
----

The header `lexy/dsl/punctuator.hpp` defines common punctuator literals.
They are equivalent to a literal matching the specified character.

=== Character classes

[discrete]
==== `lexy::dsl::eof`

.`lexy/dsl/eof.hpp`
----
eof : Token
----

The `eof` token matches EOF.

Matches::
  Only if the reader is at the end of the input. It does not consume anything (it can't).
Error::
  `lexy::expected_char_class` with the name `EOF`.

[discrete]
==== `lexy::dsl::newline`

.`lexy/dsl/newline.hpp`
----
newline : Token
----

The `newline` token matches a newline.

Matches::
   `\n` or `\r\n`, which is consumed.
Error::
  `lexy::expected_char_class` with the name `newline`.

[discrete]
==== `lexy::dsl::eol`

.`lexy/dsl/newline.hpp`
----
eol : Token
----

The `eol` token matches an end-of-line (EOL).

Matches::
  `\n` or `\r\n`, which is consumed.
  Also matches EOF, which is not consumed.
Error::
  `lexy::expected_char_class` with the name `EOL`.

[discrete]
==== `lexy::dsl::ascii::*`

.`lexy/dsl/ascii.hpp`
----
namespace ascii
{
    control : Token // 0x00-0x1F, 0x7F

    blank       : Token // ' ' (space character) or '\t'
    newline     : Token // '\n' or '\r'
    other_space : Token // '\f' or '\v'
    space       : Token // `blank` or `newline` or `other_space`

    digit : Token // 0-9

    lower                  : Token // a-z
    upper                  : Token // A-Z
    alpha                  : Token // `lower` or `upper`
    alpha_underscore       : Token // `lower` or `upper` or '_'
    alnum                  : Token // `alpha` or `digit`
    alpha_digit            : Token // `alpha` or `digit` (same as above)
    alpha_digit_underscore : Token // `alpha` or `digit` or '_'

    punct : Token // One of: !"#$%&'()*+,-./:;<=>?@[\]^_`{|}~

    graph : Token // `alpha_digit` or `punct`
    print : Token // `graph` or ' ' (space characters)

    character : Token // 0x00-0x7F
}
----

All tokens defined in `lexy::dsl::ascii` match one of the categories of ASCII characters.

Matches::
  Matches and consumes one of the set of ASCII characters indicated in the comments.
Errors::
  A `lexy::expected_char_class` error with name `ASCII.<token>`, where `<token>` is the name of the token.

NOTE: Every ASCII character except for the space character is in exactly one of `control`, `lower`, `upper`, `digit` or `punct`.

[discrete]
==== `lexy::dsl::code_point`

.`lexy/dsl/code_point.hpp`
----
code_point : Token

code_point.capture() : Rule
----

The `code_point` token will match and consume a well-formed Unicode code point according to the encoding of the input.
If `code_point.capture()` is used, the consumed code point will be produced as value.

Requires::
  The encoding of the input is `lexy::ascii_encoding`, `lexy::utf8_encoding`, `lexy::utf16_encoding`, or `lexy::utf32_encoding`.
Matches::
  Matches and consumes all code units of the next code point.
  For ASCII and UTF-32 this is only one, but for UTF-8 and UTF-16 it can be multiple code units.
  If the code point is too big or a UTF-16 surrogate, it fails.
  For UTF-8, it also fails for overlong sequences.
Value::
  If `.capture()` was called, it will produce the matched code point as a `lexy::code_point`.
Errors::
  If it could not match a valid code point, it fails with a `lexy::expected_char_class` error with name `<encoding>.code_point`.

[%collapsible]
.Example
====
[source,cpp]
----
// Match and capture one arbitrary code point.
dsl::code_point.capture()
----
====

TIP: If you want to match a specific code point, use a literal rule instead.
This rule is useful for matching things like string literals that can contain arbitrary code points.

[discrete]
==== `lexy::dsl::operator-`

.`lexy/dsl/minus.hpp`
----
token - except : Token
----

The minus rule matches the given token, but only if `except` does not match on the input the rule has consumed.

Requires::
  `except` is a token.
Matches::
  Matches and consumes whatever `token` match and consume.
  Then matches `except` on the same input.
  Matching fails if `except` matches the entire input consumed by the token.
Errors::
  Whatever errors are raised if `token` is not matched.
  A generic error with tag `lexy::minus_failure` if `except` has matched.

TIP: Use a minus rule to exclude characters from a character class; e.g. `lexy::dsl::code_point - lexy::dsl::ascii::control` matches all code points except control characters.

NOTE: Minus rules can be chained. This is equivalent to specifying an alternative for `except`.

WARNING: `except` has to match _everything_ the rule has consumed before; partial matches don't count.
Use `token - (except + lexy::dsl::any)` if you want to allow a partial match.

[discrete]
==== `lexy::dsl::token`

.`lexy/dsl/token.hpp`
----
token(rule) : Token
----

The `token` rule turns an arbitrary rule into a token by parsing it and discarding all values it has produced.

Matches::
  Whatever `rule` matches, which will be consumed.
Error::
  A generic error with tag `lexy::missing_token` if the `rule` did not match.

NOTE: While `token()` is optimized to prevent any overhead created by constructing values that are later discarded,
it still should only be used when required.

=== Values

The following rules are used to produce additional values without any additional matching.

[discrete]
==== `lexy::dsl::value_*`

.`lexy/dsl/value.hpp`
----
value_c<Value> : Rule
value_f<Fn>    : Rule
value_t<T>     : Rule
value_str<Str> : Rule

LEXY_VALUE_STR(Str) : Rule
----

The `value_*` rules create a constant value without parsing anything.

Requires::
  * `Value` is any constant.
  * `Fn` is a pointer to a function taking no arguments.
  * `T` is a default-constructible type.
  * `Str` is a string literal.
Matches::
  Any input, but does not consume anything.
Value::
  `value_c`::: The specified constant.
  `value_f`::: The result of invoking the function.
  `value_t`::: A default constructed object of the specified type.
  `value_str`::: The string literal as a pointer, followed by its size.
Error::
  n/a (it does not fail)

TIP: Use the `value_*` rules only to create symmetry between different branches.
Everything they do, can also be achieved using callbacks, which is usually a better solution.

WARNING: The function might not be called or the object might not be constructed in all situations. You cannot rely on their side effects.

NOTE: `value_str<Str>` requires C++20 support for extended NTTPs.
Use the `LEXY_VALUE_STR(Str)` macro if your compiler does not support them.

[discrete]
==== `lexy::dsl::nullopt`

.`lexy/dsl/option.hpp`
[source,cpp]
----
namespace lexy
{
    struct nullopt
    {
        template <typename T>
        constexpr operator T() const;
    };
}
----

The `lexy::nullopt` type represents an empty optional.
It is implicitly convertible to any type that has a default constructor (`T()`), a dereference operator (`*t`), and a contextual conversion to `bool` (`if (t)`).
Examples are pointers or `std::optional`.
The conversion operator returns a default constructible object, i.e. an empty optional.

.`lexy/dsl/option.hpp`
----
nullopt : Rule
----

The `nullopt` rule produces a value of type `lexy::nullopt` without parsing anything.

Matches::
  Any input, but does not consume anything.
Value::
  An object of type `lexy::nullopt`.
Error::
  n/a (it does not fail)

NOTE: It is meant to be used for symmetry with together with the `opt()` rule.

[discrete]
==== `lexy::dsl::label` and `lexy::dsl::id`

.`lexy/dsl/label.hpp`
[source,cpp]
----
namespace lexy
{
    template <typename Tag>
    struct label
    {
        // only if Tag::value is well-formed
        consteval operator auto() const
        {
            return Tag::value;
        }
    };

    template <auto Id>
    using id = label<std::integral_constant<int, Id>>;
}
----

.`lexy/dsl/label.hpp`
----
label<Tag> : Rule
id<Id>     : Rule
----

The `label` and `id` rules are used to disambiguate between two branches that create otherwise the same values but should resolve to different callbacks.
They simply produce the empty tag object or the id to differentiate them without parsing anything.

Requires::
  * `Tag` is any type.
  * `Id` is an integer constant.
Matches::
  Any input, but does not consume anything.
Value::
  `label<Tag>`::: A `lexy::label<Tag>` object.
  `id<Id>`::: A `lexy::id<Id>` object.
Error::
  n/a (it does not fail)

.`lexy/dsl/label.hpp`
----
label<Tag>(rule)   : Rule   = label<Tag> + rule
label<Tag>(branch) : Branch = /* as above, except as branch */

id<Id>(rule)   : Rule   = id<Id> + rule
id<Id>(branch) : Branch = /* as above, except as branch */
----

For convenience, `label` and `id` have function call operators.
They produce the label/id and then parse the rule.

[discrete]
==== `lexy::dsl::capture`

.`lexy/dsl/capture.hpp`
----
capture(rule)   : Rule
capture(branch) : Branch
----

The `capture()` rule takes an arbitrary rule and parses it, capturing everything it has consumed into a `lexy::lexeme`.
It is a branch if given a branch.

Branch Condition::
  The branch condition is whatever `branch` uses as a branch condition.
Matches::
  Matches and consumes whatever `rule` matches.
Values::
  A `lexy::lexeme` which begins at the original reader position and ends at the reader position after `rule` has been parsed,
  followed by any other values produced by parsing the `rule` in the same order.
Errors::
  All errors raised by `rule`. It cannot fail itself.

[%collapsible]
.Example
====
[source,cpp,id=77jfM5]
----
// Captures the entire input.
dsl::capture(dsl::any)
----
====

[discrete]
==== `lexy::dsl::position`

.`lexy/dsl/position.hpp`
----
position : Rule
----

The `position` rule creates as its value an iterator to the current reader position without consuming any input.

Matches::
  Any input, but does not consume anything.
Value::
  An iterator to the current position of the reader.
Error::
  n/a (it does not fail)

[%collapsible]
.Example
====
[source,cpp,id=Wh86vn]
----
// Parses the entire input and returns the final position.
dsl::any + dsl::position
----
====

TIP: Use `position` when creating an AST whose nodes are annotated with their original source position.

=== Errors

The following rules are used to customize/improve error messages or recover from errors.

[discrete]
==== `.error<Tag>`

----
token.error<Tag> : Token
----

The `error` member on tokens changes the error that is raised when a token failed.

Matches::
  Matches and consumes what `token` matches.
Error::
  A generic error with the specified `Tag`.

TIP: It is useful for tokens such as `dsl::token()` and `operator-`, where the result is a generic tag such as `lexy::missing_token` or `lexy::minus_failure`.

[discrete]
==== `lexy::dsl::error`

.`lexy/dsl/error.hpp`
----
error<Tag>       : Branch
error<Tag>(rule) : Branch
----

The `error` rule always fails and produces an error with the given tag.
For the second version, the rule is matched first to determine the error range.

Branch Condition::
  Branch is always taken.
Matches::
  Nothing and always fails.
Error::
  An error object of the specified `Tag`.
  If the optional `rule` is given, it will be matched (without producing values or errors).
  If it matched successfully, the previous and new reader position will be used to determine the error range.
  Otherwise, the error has no range.

TIP: Use it as the final branch of a choice rule to customize the `lexy::exhausted_choice` error.

[discrete]
==== `lexy::dsl::require` and `lexy::dsl::prevent`

.`lexy/dsl/error.hpp`
----
require(rule).error<Tag> : Rule
prevent(rule).error<Tag> : Rule
----

The `require` and `prevent` rules can be used to lookahead and fail if the input matches or does not match the token.

Matches::
  Both match the `rule` without consuming input (or producing values or errors).
  `require` fails if the `rule` did not match; `rule` fails if it did.
Error::
  An error object of the specified `Tag`.

[%collapsible]
.Example
====
[source,cpp,id=n7zM4d]
----
// Parses a sequence of digits but raises an error with tag `forbidden_leading_zero` if a zero is followed by more digits.
// Note: this is already available as `dsl::digits<>.no_leading_zero()`.
dsl::zero >> dsl::prevent<forbidden_leading_zero>(dsl::digits<>)
    | dsl::digits<>
----
====

TIP: Use `prevent` together with `times` to prevent the rule from matching more than the specified number of times.

[discrete]
==== `lexy::dsl::try_`

.`lexy/dsl/recover.hpp`
----
try_(rule) : Rule
try_(rule, recovery_rule) : Rule
----

The `try_` rule matches and consumes `rule`.
If that fails, it recovers from the error and continues as if it didn't fail.
The first overload recovers by doing nothing, the second recovers by parsing the recovery rule.

Matches::
  Matches and consumes `rule`.
  If that fails, matches and consumes `recovery_rule`.
Values::
  All values produced by `rule` if `rule` was parsed successfully.
  All values produced by `recovery_rule` otherwise.
Error::
  All errors raised by `rule` or `recovery_rule`.

[discrete]
==== `lexy::dsl::find`

.`lexy/dsl/recover.hpp`
----
find(token_1, ..., token_n) : Rule
find(token_1, ..., token_n).limit(token_1, ..., token_n) : Rule
----

The `find` rule is designed to be used as a recovery rule.
It matches and consumes everything until it finds one of the tokens.
The tokens are not consumed.

If a limit is specified, recovery fails if the limiting tokens are found first.
The limiting tokens are not consumed either.

[discrete]
==== `lexy::dsl::recover`

.`lexy/dsl/recover.hpp`
----
recover(branch_1, ..., branch_n) : Rule
recover(branch_1, ..., branch_n).limit(branch_1, ..., branch_n) : Rule
----

The `recover` rule is designed to be used as a recovery rule.
It matches and consumes everything until one of the recovery branches match.
It then matches and consumes the branch.

If a limit is specified, recovery fails if the limiting tokens are found first.
The limiting tokens are not consumed.

=== Branch conditions

The following rules are designed to be used as the condition of an `operator>>`.
They have no effect if not used in a context that requires a branch.

[discrete]
==== `lexy::dsl::else_`

.`lexy/dsl/branch.hpp`
----
else_ : Branch
----

If `else_` is used as a condition, that branch will be taken unconditionally.
It must be used as a last alternative in a choice.

[discrete]
==== `lexy::dsl::peek`

.`lexy/dsl/peek.hpp`
----
peek(rule) : Branch
----

The `peek` branch is taken if `rule` matches, but does not consume it.

CAUTION: Automatic whitespace skipping is disabled while determining whether `rule` matches.

CAUTION: Long lookahead can slow down parsing speed due to backtracking.

[discrete]
==== `lexy::dsl::peek_not`

.`lexy/dsl/peek.hpp`
----
peek_not(rule) : Branch
----

The `peek_not()` branch is taken if `rule` does not match, but does not consume it.

CAUTION: Automatic whitespace skipping is disabled while determining whether `rule` matches.

CAUTION: Long lookahead can slow down parsing speed due to backtracking.

[discrete]
==== `lexy::dsl::lookahead`

.`lexy/dsl/lookahead.hpp`
----
lookahead(needle, end) : Branch
----

The `lookahead` branch is taken if lookahead finds `needle` before `end` is found, which must both be tokens.
No characters are consumed.

CAUTION: Long lookahead can slow down parsing speed due to backtracking.

=== Branches

[discrete]
==== `lexy::dsl::operator+`

.`lexy/dsl/sequence.hpp`
----
rule + rule   : Rule
token + token : Branch
----

A sequence rule matches multiple rules one after the other.
It is a branch if it is a sequence of tokens.

Branch Condition::
  Branch is only taken if all tokens match in order.
Matches::
  Matches and consume the first rule, then matches and consumes the second rule, and so on.
  Only succeeds if all of them succeed.
Values::
  All the values produced by the rules in the same order as they were matched.
Errors::
  Whatever errors are raised by the individual rules.

[discrete]
==== `lexy::dsl::operator>>`

.`lexy/dsl/branch.hpp`
----
branch >> rule : Branch
----

The `operator>>` is used to turn a rule into a branch by giving it a branch condition, which must be a branch itself.
If the branch is used as a normal rule, it first matches the condition followed by the rule.
If it is used in a context that requires a branch, the branch is checked to determine whether it should be taken.

Branch Condition::
  Whatever `branch` uses as branch condition.
Matches::
  Matches and consume the branch, then matches and consumes the `rule`.
  Only succeeds if all of them succeed.
Values::
  All the values produced by the branch and rule in the same order as they were matched.
Errors::
  Whatever errors are raised by the individual branch and rule.

[discrete]
==== `lexy::dsl::if_`

.`lexy/dsl/if.hpp`
----
if_(branch) : Rule
----

The `if_` rule matches a branch only if its condition matches.

Matches::
  First matches the branch condition.
  If that succeeds, consumes it and matches and consumes the rest of the branch.
  Otherwise, consumes nothing and succeeds anyway.
Values::
  Any values produced by the branch.
Errors::
  Any errors produced by the branch.
  It will only fail after the condition has been matched.

[%collapsible]
.Example
====
[source,cpp,id=GaxjbP]
----
// Matches an optional C style comment.
dsl::if_(LEXY_LIT("/*") >> dsl::until(LEXY_LIT("*/")))
----
====

[discrete]
==== `lexy::dsl::opt`

.`lexy/dsl/opt.hpp`
----
opt(branch) : Rule = branch | else_ >> nullopt
----

The `opt` rule matches a branch only if its condition matches.
Unlike `if_`, if the branch was not taken, it produces a `lexy::nullopt`.

Matches::
  First matches the branch condition.
  If that succeeds, consumes it and matches and consumes the rest of the branch.
  Otherwise, consumes nothing and succeeds anyway.
Values::
  If the branch condition matches, any values produced by the rule.
  Otherwise, a single object of type `lexy::nullopt`.
Errors::
  Any errors produced by the branch.
  It will only fail after the condition has been matched.

[%collapsible]
.Example
====
[source,cpp,id=1vK39o]
----
// Matches an optional list of alpha characters.
// (The id<0> is just there, so the sink will be invoked on each character).
// If no items are present, it will default construct the list type.
dsl::opt(dsl::list(dsl::ascii::alpha >> dsl::id<0>))
----
====

[discrete]
==== `lexy::dsl::operator|`

.`lexy/dsl/choice.hpp`
----
branch  | branch  : Branch
----

A choice rule matches the first branch in order whose condition was matched.
It is always a branch, that is taken if any of its branches is taken.

Matches::
  Tries to match the condition of each branch in the order they were specified.
  As soon as one branch condition matches, matches and consumes that branch without ever backtracking to try another branch.
  If no branch condition matched, fails without consuming anything.
Values::
  Any values produced by the selected branch.
Errors::
  Any errors raised by the then of the selected branch.
  If no branch condition matched, a generic error with tag `lexy::exhausted_choice`.

[%collapsible]
.Example
====
[source,cpp,id=aaEnW7]
----
// A contrived example to illustrate the behavior of choice.
// Note that branch with id 1 will never be taken, as branch 0 takes everything starting with a and then fails if it isn't followed by bc.
// The correct behavior is illustrated with 2 and 3, there the branch with the longer condition is listed first.
dsl::id<0>(LEXY_LIT("a") >> LEXY_LIT("bc"))
  | dsl::id<1>(LEXY_LIT("a") >> LEXY_LIT("b"))
  | dsl::id<2>(LEXY_LIT("bc"))
  | dsl::id<3>(LEXY_LIT("b"))
----
====

NOTE: The C++ operator precedence is specified in such a way that `condition >> a | else_ >> b` works.
The compiler might warn that the precedence is not intuitive without parentheses, but in the context of this DSL it is the expected result.

TIP: Use `… | error<Tag>` to raise a custom error instead of `lexy::exhausted_choice`.

[discrete]
==== `lexy::dsl::operator/`

.`lexy/dsl/alternative.hpp`
----
token / token : Token
----

An alternative rule tries to match one of the tokens, backtracking if necessary.

Matches::
  Tries to match each token in some order.
  If one token matched, matches and consumes it.
  If multiple tokens matched, matches and consumes the longest one.
  If no token matched, fails without consuming anything.
Errors::
  A generic error with tag `lexy::exhausted_alternatives` if no token matched.

NOTE: Literals in the alternative are efficiently matched using a trie without doing backtracking.

CAUTION: Use a choice rule with a suitable condition to avoid potentially long backtracking.

=== Loops

[discrete]
==== `lexy::dsl::until`

.`lexy/dsl/until.hpp`
----
until(token)          : Token
until(token).or_eof() : Token = until(token / eof)
----

The `until` token consumes all input until the specified `token` matches, then consumes that.

Matches::
  If the closing `token` matches, consumes it and succeeds.
  Otherwise, consumes one code unit and tries again.
  If EOF is reached, fails, unless `.or_eof()` was called, in which case it also succeeds having consumed everything until the end of the input.
Errors::
  It can only fail if the reader has reached the end of the input without matching the condition.
  Then it raises the same error as raised if the condition would be matched at EOF.

[%collapsible]
.Example
====
[source,cpp,id=Yn4WTj]
----
// Matches a C style comment.
// Note that we don't care what it contains.
LEXY_LIT("/*") >> dsl::until(LEXY_LIT("*/"))
----
====

NOTE: `until` includes the `token`.

[discrete]
==== `lexy::dsl::loop`

.`lexy/dsl/loop.hpp`
----
loop(rule) : Rule

break_ : Rule
----

The `loop` rule matches the given rule repeatedly until it either fails to match or a `break_` rule was matched.

Requires::
  `rule` must not produce any values.
  `break_` must be used inside a loop.
Matches::
  While the rule matches, consumes it and repeats.
  If a `break_` is matched, parsing will stop immediately and it succeeds.
  If the rule does not match, it fails.
Values::
  No values are produced.
Errors::
  Any errors raised when the rule fails to match.

NOTE: The `loop` rule is mainly used to implement other rules.
It is unlikely that you are going to need it yourself.

WARNING: If `rule` contains a branch that will not consume any characters but does not break, `loop` will loop forever.

[discrete]
==== `lexy::dsl::while_`

.`lexy/dsl/while.hpp`
----
while_(branch) : Rule
----

The `while` rule matches a branch as long as it condition has matched.

Requires::
  `branch` must not produce any values.
Matches::
  While the branch condition matches, matches and consumes the then then repeats.
  If the branch condition does not match anymore, succeeds without consuming additional input.
Values::
  No values are produced.
Errors::
  The rule can only fail if the then of the branch fails.
  Then it will raise its error unchanged.

WARNING: If the branch does not consume any characters, `while_` will loop forever.

[discrete]
==== `lexy::dsl::while_one()`

.`lexy/dsl/while.hpp`
----
while_one(branch)  : Branch  = branch >> while_(branch)
----

The `while_one` rule matches a rule one or more times.

[discrete]
==== `lexy::dsl::do_while()`

.`lexy/dsl/while.hpp`
----
do_while(rule, condition_branch)   : Rule   = rule + while_(condition_branch >> rule)
do_while(branch, condition_branch) : Branch = branch >> while_(condition_branch >> rule)
----

The `do_while` rule matches a rule first unconditionally, and then again repeatedly while the rule matches.

[%collapsible]
.Example
====
[source,cpp,id=4dzEK7]
----
// Equivalent to `dsl::list(dsl::ascii::alpha, dsl::sep(dsl::comma))` but does not produce a value.
dsl::do_while(dsl::ascii::alpha, dsl::comma)
----
====

[discrete]
==== `lexy::dsl::sep` and `lexy::dsl::trailing_sep`

.`lexy/dsl/separator.hpp`
----
sep(branch)
sep(branch).trailing_error<Tag>
trailing_sep(branch)
----

`sep` and `trailing_sep` are used to specify a separator between repeated items; they are not rules that can be parsed directly.

Use `sep(branch)` to indicate that `branch` has to be consumed between two items.
If it would match after the last item (and the item is a branch to allow checking for it), an error is raised.
It has the tag `lexy::unexpected_trailing_separator` unless a different one was specified using `.trailing_error`.

Use `trailing_sep(branch)` to indicate that `branch` has to be consumed between two items and can occur after the final item.
If it matches after the last item, it is consumed as well.

[discrete]
==== `lexy::dsl::times`

.`lexy/dsl/times.hpp`
[source,cpp]
----
namespace lexy
{
    template <std::size_t N, typename T>
    using times = T (&)[N];

    template <typename T>
    using twice = times<2, T>;
}
----

.`lexy/dsl/times.hpp`
----
times<N>(rule)      : Rule
times<N>(rule, sep) : Rule

twice(rule)      : Rule = times<2>(rule)
twice(rule, sep) : Rule = times<2>(rule, sep)
----

The `times` rule repeats the rule `N` times with optional separator in between and collects all produced values into an array.
The `twice` rule is a convenience alias for `N = 2`.

Requires::
  The separator must not produce any values.
  All values produced by the parsing the rule must have a common type.
  In particular, the rule must only produce one value.
Matches::
  If no separator is specified, matches and consumes `rule` `N` times.
  If a separator is specified, matches and consumes `rule` `N` times, consuming the separator between two items and potentially after all items if the separator is trailing.
Values::
  Produces a single array containing `N` items which are all the values produced by each repetition.
  The typedef `lexy::times` or `lexy::twice` can be used to process that array.
Errors::
  All errors raised by matching the rule or separator.

[%collapsible]
.Example
====
[source,cpp,id=hrTKaT]
----
// Parses an IPv4 address (4 uint8_t's seperated by periods).
dsl::times<4>(dsl::integer<std::uint8_t>(dsl::digits<>), dsl::sep(dsl::period))
----
====

[discrete]
==== `lexy::dsl::list`

.`lexy/dsl/list.hpp`
----
list(rule)   : Rule
list(branch) : Branch

list(rule, sep)   : Rule
list(branch, sep) : Branch
----

The `list` rule matches a rule one or more times, optionally separated by a separator.
Values produced by the list items are forwarded to a sink callback.

Branch Condition::
  Whatever `branch` uses as branch condition.
Requires::
  The item rule must be a branch unless a non-trailing separator is used (in that case the separator can be used as condition).
  A production whose rule contains `list()` must provide a sink.
Matches::
  Matches and consumes the item rule one or more times.
  In between items and potentially after the final item, a separator is matched and consumed if provided according to its rules.
  If the separator is provided and non-trailing, the existence of a separator determines whether or not the rule should be matched again.
  Otherwise, the branch condition of the branch rule or an added else branch of the choice rule is used to determine that.
Values::
  Only a single value, which is the result of the finished sink.
  Every time the item rule is parsed, all values it produces are passed to the sink which is invoked once per iteration.
  If the separator is captured, its lexeme is also passed to the sink, but in a separate invocation.
Errors::
  All errors raised when parsing the item rule or separator.

[%collapsible]
.Example
====
[source,cpp,id=sE873v]
----
// Parses a list of integers seperated by (a potentially trailing) comma.
// As the separator is trailing, it cannot be used to determine the end of the list.
// As such we peek whether the input contains a digit in our item condition.
// The sink is invoked with each integer.
dsl::list(dsl::peek(dsl::digit<>) >> dsl::integer<int>(dsl::digits<>),
          dsl::trailing_sep(dsl::comma))
----
====

TIP: Use one of the bracketing rules if your list item does not have an easy condition and the list is surrounded by given tokens anyway.

[discrete]
==== `lexy::dsl::opt_list`

.`lexy/dsl/list.hpp`
----
opt_list(branch)      : Rule
opt_list(branch, sep) : Rule
----

The `opt_list` rule matches a rule zero or more times, optionally separated by a separator.
Values produced by the list items are forwarded to a sink callback.

Requires::
  The item rule must be a branch.
  A production whose rule contains `opt_list()` must provide a sink.
Matches::
  Checks whether the item rule would match using its branch condition.
  If it does, matches and consumes `list(branch, sep)`.
  Otherwise, consumes nothing and succeeds.
Values::
  If the list is non-empty, the result of the sink produced by parsing the `list()` rule.
  Otherwise, the result of a sink that is immediately finished without invoking it once.
Errors::
  If the list is non-empty, all errors raised by parsing the `list()` rule.

[discrete]
==== `lexy::dsl::combination`

.`lexy/dsl/combination.hpp`
----
combination(branch1, branch2, ...) : Rule
combination(branch1, branch2, ...).duplicate_error<Tag> : Rule
combination(branch1, branch2, ...).missing_error<Tag> : Rule
----

The `combination` rule matches each of the sub-rules exactly once but in any order.
Values produced by the rules are forwarded to a sink.

Requires::
  A production whose rule contains `combination()` must provide a sink.
Matches::
  Matches and consumes all rules in an arbitrary order.
  This is done by parsing the choice created from the branches exactly `N` times.
  Branches that have already been taken are not excluded on future iterations.
  If they are taken again, the rule fails.
Values::
  Only a single value, which is the result of the finished sink.
  All values produced by the branches are passed to the sink which is invoked once per iteration.
Errors::
  All errors raised by parsing the branches.
  If no branch is matched, but there are still missing branches,
  a generic error with the tag specified using `missing_error` is raised, or `lexy::exhausted_choice` if there was none.
  If a branch is matched twice,
  a generic error with the tag specified using `duplicate_error` is raised, or `lexy::combination_duplicate` if there was none.

[%collapsible]
.Example
====
[source,cpp,id=bjKqvj]
----
// Matches 'a', 'b', or 'c', in any order.
dsl::combination(dsl::lit_c<'a'>, dsl::lit_c<'b'>, dsl::lit_c<'c'>)
----
====

WARNING: The branches are tried in order. If an earlier branch always takes precedence over a later one, the combination can never be successful.

[discrete]
==== `lexy::dsl::partial_combination`

.`lexy/dsl/combination.hpp`
----
partial_combination(branch1, branch2, ...) : Rule
partial_combination(branch1, branch2, ...).duplicate_error<Tag> : Rule
----

The `partial_combination` rule matches each of the sub-rules at most once but in any order.
Values produced by the rules are forwarded to a sink.

Requires::
  A production whose rule contains `partial_combination()` must provide a sink.
Matches::
  Matches and consumes a subset of the rules in an arbitrary order.
  This is done by parsing the choice created from the branches exactly `N` times.
  Branches that have already been taken are not excluded on future iterations.
  If they are taken again, the rule fails.
  If no branch is taken, the rule succeeds.
Values::
  Only a single value, which is the result of the finished sink.
  All values produced by the branches are passed to the sink which is invoked once per iteration.
Errors::
  All errors raised by parsing the branches.
  If a branch is matched twice,
  a generic error with the tag specified using `duplicate_error` is raised, or `lexy::combination_duplicate` if there was none.

[%collapsible]
.Example
====
[source,cpp,id=85dv9W]
----
// Matches a subset of 'a', 'b', or 'c', in any order.
dsl::partial_combination(dsl::lit_c<'a'>, dsl::lit_c<'b'>, dsl::lit_c<'c'>)
----
====

WARNING: The branches are tried in order. If an earlier branch always takes precedence over a later one, the combination can never be successful.

=== Productions

Every rule is owned by a production.
The following rules allow interaction with other productions.

[discrete]
==== `lexy::dsl::p` and `lexy::dsl::recurse`

.`lexy/dsl/production.hpp`
----
p<Production> : Rule or Branch
recurse<Production> : Rule
----

The `p` and `recurse` rules parses the rule of another production.
The `p` rule is a branch, if the rule of the other production is a branch.

Requires::
  For `p`, the `Production` is a complete type at the point of the rule definition.
  The `recurse` rule has no such limitations.
Branch Condition::
  Whatever the production's rule uses as a branch condition.
Matches::
  Matches and consumes `Production::rule`.
  If `Production` inherits from `lexy::token_production`,
  automatic whitespace is skipped afterwards by matching and consuming the `lexy::dsl::whitespace` rule.
Values::
  A single value, which is the result of parsing the production.
  All values produced by parsing its rule are forwarded to the productions value callback.
Errors::
  If matching fails, `Production::rule` will raise an error which is handled in the context of `Production`.
  This results in a failed result object, which is converted to our result type and returned.

[%collapsible]
.Example
====
[source,cpp,id=oj9T3n]
----
// Parse a sub production followed by an exclamation mark.
dsl::p<sub_production> + dsl::lit_c<'!'>
----
====

TIP: While `recurse` can be used to implement direct recursion (e.g. `prefix >> dsl::p<current_production> | dsl::else_ >> end` to match zero or more `prefix` followed by `end`), it is better to use loops instead.

WARNING: Left recursion will create an infinite loop.

CAUTION: If a production is parsed while whitespace skipping has been disabled using `lexy::dsl::no_whitespace()`,
it is temporarily re-enabled while `Production::rule` is parsed.
If whitespace skipping has been disabled because the parent production inherits from `lexy::token_production`,
whitespace skipping is still disabled while parsing `Production::rule`.

[discrete]
==== `lexy::dsl::return_`

.`lexy/dsl/return.hpp`
----
return_ : Rule
----

Conceptually, each production has an associated function that parses the specified rule.
The `return_` rule will exit that function early, without parsing subsequent rules.

Requires::
  It must not be used inside loops.
Matches::
  Any input, but does not consume anything.
  Subsequent rules are not matched further.
Values::
  It does not produce any values, but all values produced so far are forwarded to the callback.
Errors::
  n/a (it does not fail)

[%collapsible]
.Example
====
[source,cpp,id=zrbcaq]
----
// Match an opening parenthesis followed by 'a' or 'b'.
// If it is followed by 'b', the closing parenthesis is not matched anymore.
dsl::parenthesized(dsl::lit_c<'a'> | dsl::lit_c<'b'> >> dsl::return_)
----
====

CAUTION: When using `return_` together with the context sensitive parsing facilities, remember to pop all context objects before the return.

=== Brackets and terminator

[discrete]
==== Terminator

.`lexy/dsl/terminator.hpp`
----
terminator(branch)
terminator(branch).terminator() : Branch = branch
terminator(branch).limit(token_1, ..., token_n)
----

A terminator can be specified using `terminator()`.
The result is not a rule, but a DSL for specifying that a rule is followed by the terminator.
The terminator is defined using a branch; it is returned by calling `.terminator()`.

The terminator rules do automatic error recovery by matching and consuming input until the terminator is found.
The error recovery can be limited by calling `.limit()`, which behaves like the limit of `dsl::recover()` or `dsl::find()`.

.`lexy/dsl/terminator.hpp`
----
t(rule) : Rule = rule + t.terminator()
----

Calling `t(rule)`, where `t` is the result of a `terminator()` call, results in a rule that parses the given `rule` followed by the terminator.

.`lexy/dsl/terminator.hpp`
----
t.try_(rule) : Rule
----

Calling `t.try_(rule)`, where `t` is the result of a `terminator()` call, results in a rule that tries to parse the given `rule` followed by the terminator.
If an error occurs while parsing `rule`, recovers by discarding input until the terminator (or the recovery limit) is found.

.`lexy/dsl/terminator.hpp`
----
t.while_(rule) : Rule
t.while_one(rule) : Rule

t.opt(rule) : Rule

t.list(rule) : Rule
t.list(rule, sep) : Rule

t.opt_list(rule) : Rule
t.opt_list(rule, sep) : Rule
----

Using `t.while_()`, `t.while_one()` `t.opt()`, `t.list()`, or `t.opt_list()`, where `t` is the result of a `terminator()` call,
results in a rule that parses `while_(rule)`, `while_one(rule)`, `opt(rule)`, `list(rule)` and `opt_list(rule)`, respectively, but followed by the terminator.
The `rule` does not need to be a branch, as the terminator is used as the branch condition for the `while_()`, `opt()` and `list()` rule.

[discrete]
==== Brackets

.`lexy/dsl/brackets.hpp`
----
brackets(open_branch, close_branch)
brackets(open_branch, close_branch).open()  : Branch = open_branch
brackets(open_branch, close_branch).close() : Branch = close_branch
brackets(open_branch, close_branch).limit(token_1, ..., token_n)
----

A set of open and close brackets can be specified using `brackets()`.
The result is not a rule, but a DSL for specifying that a rule is surrounded by brackets.
The open and close brackets are defined using branches; they are returned by calling `.open()` and `.close()`.

The bracket rules do automatic error recovery by matching and consuming input until the closing bracket is found.
The error recovery can be limited by calling `.limit()`, which behaves like the limit of `dsl::recover()` or `dsl::find()`.

.`lexy/dsl/brackets.hpp`
----
b(rule) : Branch = b.open() >> rule + b.close()
----

Calling `b(rule)`, where `b` is the result of a `brackets()` call, results in a rule that parses the given `rule` surrounded by brackets.
The rule is a branch that uses the opening bracket as a branch condition.

.`lexy/dsl/terminator.hpp`
----
b.try_(rule) : Rule
----

Calling `b.try_(rule)`, where `t` is the result of a `brackets()` call, results in a rule that tries to parse the given `rule` surrounded by brackets.
If an error occurs while parsing `rule`, recovers by discarding input until the closing bracket (or the recovery limit) is found.

.`lexy/dsl/brackets.hpp`
----
b.while_(rule) : Branch
b.while_one(rule) : Branch

b.opt(rule) : Branch

b.list(rule) : Branch
b.list(rule, sep) : Branch

b.opt_list(rule) : Branch
b.opt_list(rule, sep) : Branch
----

Using `b.while_()`, `b.while_one()` `b.opt()`, `b.list()`, or `b.opt_list()`, where `b` is the result of a `brackets()` call, results in a branch that parses `while_(rule)`, `while_one(rule)`, `opt(rule)`, `list(rule)` and `opt_list(rule)`, respectively, but surrounded as brackets.
The `rule` does not need to be a branch, as the closing brackets is used as the branch condition for the `while_()`, `opt()` and `list()` rule.

.`lexy/dsl/brackets.hpp`
----
round_bracketed  = brackets(lit_c<'('>, lit_c<')'>)
square_bracketed = brackets(lit_c<'['>, lit_c<']'>)
curly_bracketed  = brackets(lit_c<'{'>, lit_c<'}'>)
angle_bracketed  = brackets(lit_c<'<'>, lit_c<'>'>)

parenthesized = round_bracketed
----

Common sets of open and close brackets are pre-defined.

[%collapsible]
.Example
====
[source,cpp,id=G9MPKh]
----
// Parses a list of integers seperated by (a potentially trailing) comma surrounded by parentheses.
// The same example without the parentheses was also used for list,
// but we required a list condition that needed to perform lookahead.
// Now, the closing parentheses is used as the condition and we don't need to lookahead.
dsl::parenthesized.list(dsl::integer<int>(dsl::digits<>),
                        dsl::trailing_sep(dsl::comma))
----
====

=== Numbers

The facilities for parsing integers are split into the digit token, which do not produce any values,
and the `integer` rule, which matches a digit token and converts it into an integer.
The integer conversion has to be done during and parsing and not as a callback, as overflow creates a parse error.

[discrete]
==== Base

.`lexy/dsl/digit.hpp`
[source,cpp]
----
namespace lexy::dsl
{
    struct binary;
    struct octal;
    struct decimal;
    struct hex_lower;
    struct hex_upper;
    struct hex;
}
----

The set of allowed digits and their values is specified using a `Base`, which is a policy class passed to the rules.

`binary`::
  Matches the base 2 digits `0` and `1`.
`octal`::
  Matches the base 8 digits `0-7`.
`decimal`::
  Matches the base 10 digits `0-9`. If no base is specified, this is the default.
`hex_lower`::
  Matches the lower-case base 16 digits `0-9` and `a-f`.
`hex_upper`::
  Matches the upper-case base 16 digits `0-9` and `A-F`.
`hex`::
  Matches the base 16 digits `0-9`, `A-F`, and `a-f`.

[discrete]
==== `lexy::integer_traits`

.`lexy/dsl/integer.hpp`
[source,cpp]
----
namespace lexy
{
    template <typename T>
    struct integer_traits
    {
        using type = T;

        static constexpr bool is_bounded;

        template <int Radix>
        static constexpr std::size_t max_digit_count;

        template <int Radix>
        static constexpr void add_digit_unchecked(type& result, unsigned digit);
        template <int Radix>
        static constexpr bool add_digit_checked(type& result, unsigned digit)
    };

    template <>
    struct integer_traits<lexy::code_point>;

    template <typename T>
    struct unbounded
    {};
    template <typename T>
    struct integer_traits<unbounded<T>>
    {
        using type                       = typename integer_traits<T>::type;
        static constexpr bool is_bounded = false;

        template <int Radix>
        static constexpr void add_digit_unchecked(type& result, unsigned digit);
    };
}
----

The `lexy::integer_traits` are used for parsing an integer.
It controls its maximal value and abstracts away the required integer operations.

The `type` member is the actual type that will be returned by the parse operation. It is usually `T`.
The parsing algorithm does not require that `type` is an integer type, it only needs to have a constructor that initializes it from an `int`.
If `is_bounded` is `true`, parsing requires overflow checking.
Otherwise, parsing does not require overflow checking and `max_digit_count` and `add_digit_checked` are not required.
`max_digit_count` returns the number of digits necessary to express the bounded integers maximal value in the given radix.
It must be bigger than `1`.
`add_digit_unchecked` and `add_digit_checked` add `digit` to result by doing the equivalent of `result = result * Radix + digit`.
The `_checked` version returns `true` if that has lead to an integer overflow.

The primary template works with any integer type and there is a specialization for `lexy::code_point`.
By wrapping your integer type in `lexy::unbounded`, you can disable bounds checking during parsing.
It specialization of `lexy::integer_traits` is built on top of the specialization of `lexy::integer_traits<T>`,
but disables all bounds checking.
You can specialize `lexy::integer_traits` for your own integer types.

[discrete]
==== `lexy::dsl::zero`

.`lexy/dsl/digit.hpp`
----
zero : Token
----

The `zero` token matches the zero digit.

Matches::
    Matches and consumes the zero digit `0`.
Errors::
    Raises a `lexy::expected_char_class` error with the name `digit.zero`.

[discrete]
==== `lexy::dsl::digit`

.`lexy/dsl/digit.hpp`
----
digit<Base> : Token
----

The `digit` token matches a digit of the specified base or `decimal` if no base was specified.

Matches::
    Matches and consumes any of the valid digits of the base.
Errors::
    Raises a `lexy::expected_char_class` error with the name `digit.<base>`, where `<base>` is `binary`, `hex-lower`, etc.

[discrete]
==== `lexy::dsl::digits`

.`lexy/dsl/digit.hpp`
----
digits<Base> : Token

digits<Base>.sep(token)        : Token
digits<Base>.no_leading_zero() : Token
----

The `digits` token matches a non-empty sequence of digits in the specified base or `decimal` if no base was specified.
Calling `.sep()` allows adding a digit separator token that can be present at any point between two digits, but is not required.
Calling `.no_leading_zero()` raises an error if one or more leading zeros are encountered.
The calls to `.sep()` and `.no_leading_zero()` can be chained.

Matches::
  Matches and consumes one or more digits of the specified base.
  If a separator was added, it tries to match it after every digit.
  It is consumed if it was matched, but it does not fail if no separator was present.
  If a separator is matched without a following digit, it fails.
  If `.no_leading_zero()` was called, fails if the first digit was zero and it is followed by another digit or separator.
  If it could not match any more digits after the initial one, matching succeeds.
Errors::
  All errors raised by `digit<Base>`, which can only happen for the initial digit.
  Raises a generic error with tag `lexy::forbidden_leading_zero` if a leading zero was matched.

[%collapsible]
.Example
====
[source,cpp,id=Kq1vez]
----
// Matches upper-case hexadecimal digits seperated by ' without leading zeroes.
dsl::digits<dsl::hex_upper>.sep(dsl::digit_sep_tick).no_leading_zero()
----
====

NOTE: The separator can be placed at any point between two digits.
There is no validation of rules to ensure it is a thousand separator or similar conventions.

'''

.`lexy/dsl/digit.hpp`
----
digit_sep_underscore : Token = lit<"_">
digit_sep_tick       : Token = lit<"'">
----

For convenience, two common digit separators `_` and `'` are predefined as `digit_sep_underscore` and `digit_sep_tick` respectively.
However, the digit separator can be an arbitrarily complex token.

[discrete]
==== `lexy::dsl::n_digits`

.`lexy/dsl/digit.hpp`
----
n_digits<N, Base> : Token

n_digits<N, Base>.sep(token) : Token
----

The `n_digits` token matches exactly `N` digits in the specified base or `decimal` if no base was specified.
Calling `.sep()` allows adding a digit separator token that can be present at any point between two digits, but is not required.

Matches::
  Matches and consumes exactly `N` digits of the specified base.
  If a separator was added, it tries to match it after every digit.
  It is consumed if it was matched, but it does not fail if no separator was present.
  If a separator is matched without a following digit, it fails.
  Separators do not count towards the digit count.
Errors::
  All errors raised by `digit<Base>`, which can happen if less than `N` digits are available.
  Raises a generic error with tag `lexy::forbidden_leading_zero` if a leading zero was matched.

[%collapsible]
.Example
====
[source,cpp,id=1YcrGa]
----
// Matches 4 upper-case hexadecimal digits seperated by '.
dsl::n_digits<4, dsl::hex_upper>.sep(dsl::digit_sep_tick)
----
====

[discrete]
==== `lexy::dsl::integer`

.`lexy/dsl/integer.hpp`
----
integer<T, Base>(token) : Rule
----

The `integer` rule converts the lexeme matched by the `token` into an integer of type `T` using the given base.
The `Base` can be omitted if the token is `digits` or `n_digits`.
It will then be deduced from the token.

Matches::
  Matches and consumes what `token` matches.
Values::
  An integer of type `T` that is created by the characters the token has consumed.
  If the token matches characters that are not valid digits of the base (e.g. a digit separator), those characters are skipped.
  Otherwise, the character is converted to a digit and added to the resulting integer using the `lexy::integer_traits`.
Errors::
  Any errors raised by matching the token.
  If the integer type `T` is bounded and the integer value would overflow, a generic error with tag `lexy::integer_overflow` is raised.

[%collapsible]
.Example
====
[source,cpp,id=6ThWPn]
----
// Matches upper-case hexadecimal digits seperated by ' without leading zeroes.
// Converts them into an integer, the base is deduced from the token.
dsl::integer<int>(dsl::digits<dsl::hex_upper>
                        .sep(dsl::digit_sep_tick).no_leading_zero())
----
====

[discrete]
==== `lexy::dsl::code_point_id`

.`lexy/dsl/integer.hpp`
----
code_point_id<N, Base> : Rule = integer<lexy::code_point>(n_digits<N, Base>) // approximatively
----

The `code_point_id` rule is a convenience rule that parses a code point.
It matches `N` digits in the specified base, which defaults to `hex`, and converts it into a code point.

Matches::
  Matches and consumes exactly `N` digits of the specified base.
Values::
  The `lexy::code_point` that is specified using those digits.
Errors::
  The same error as `digit<Base>` if fewer than `N` digits are available.
  A generic error with tag `lexy::invalid_code_point` if the code point value would exceed the maximum code point.

[discrete]
==== `lexy::dsl::plus_sign`, `lexy::dsl::minus_sign`, and `lexy::dsl::sign`

.`lexy/dsl/sign.hpp`
----
plus_sign  : Rule
minus_sign : Rule

sign : Rule
----

The `plus_sign`, `minus_sign`, and `sign` rule match an optional sign.

Matches::
  `plus_sign`:::
    Matches and consumes a `+` character, if there is one.
  `minus_sign`:::
    Matches and consumes a `-` character, if there is one.
  `sign`:::
    Matches and consumes a `+` or `-` character, if there is one.
Values:::
  If a `+` sign was consumed, the value is `+1`.
  If a `-` sign was consumed, the value is `-1`.
  If no sign was consumed, the value is `+1`.
Errors::
  n/a (they don't fail)

[%collapsible]
.Example
====
[source,cpp,id=7exP55]
----
// Parse a decimal integer with optional minus sign.
dsl::minus_sign + dsl::integer<int>(dsl::digits<>)
----
====

TIP: The callback `lexy::as_integer` takes the value produced by the sign rules together with an integer produced by the `integer` rule and negates it if necessary.

=== Keywords and Identifiers

[discrete]
===== `lexy::dsl::identifier()`

.`lexy/dsl/identifier.hpp`
----
identifier(token) : Rule = identifier(token, token)
identifier(leading_token, trailing_token) : Rule
----

The `dsl::identifier()` rule matches an identifier, which is the leading token followed by zero or more occurrences of the trailing token.
If the identifier is reserved (see below), an error is raised.

Requires::
  `leading_token` and `trailing_token` are tokens.
Matches::
  Matches and consumes the same as `dsl::token(leading_token + dsl::while_(trailing_token))`.
Errors::
  All errors raised by matching the `leading_token`.
  If the resulting identifier is reserved (see below), raises a generic error with tag `lexy::reserved_identifier`.
Values::
  Captures everything it consumes to produce a `lexy::lexeme`, as-if done by `dsl::capture()`.

.`lexy/dsl/identifier.hpp`
----
id.reserve(rule)             : Rule <1>
id.reserve_prefix(rule)      : Rule <2>
id.reserve_containing(rule)  : Rule <3>
----
<1> Reserves an identifier if it matches `rule` exactly.
<2> Reserves an identifier if it starts with `rule`.
<3> Reserves an identifier if a substring matches `rule`.

The reserved identifiers (e.g. keywords) can be specified using these function overloads.
After matching an identifier, the lexeme is checked against the rules specified here.
All variants are variadic to specify multiple rules at once, and they can be chained where `id.reserve(a).reserve(b).reserve(c)` is equivalent to `id.reserve(a, b, c)`.

NOTE: This design allows to use a different set of reserved identifiers in different places in the grammar to handle contextual keywords.

.`lexy/dsl/identifier.hpp`
----
id.leading()  : Token <1>
id.trailing() : Token <2>
id.pattern()  : Token <3>
----
<1> Returns the leading token of the identifier.
<2> Returns the trailing token of the identifier.
<3> Returns a token that matches and consumes `dsl::token(id.leading() + dsl::while_(id.trailing()))`.
    It ignores all rules for reserved identifiers and does not produce a value.

[discrete]
==== `lexy::dsl::keyword()`

.`lexy/dsl/identifier.hpp`
----
keyword<Str>(id) : Token

LEXY_KEYWORD(Str, id) : Token
----

The `dsl::keyword()` token matches a keyword, i.e. a specific identifier.

Requires::
  `Str` is a string and `id` is an identifier rule.
Matches::
  Matches and consumes `id.pattern()` (i.e. the identifier but without handling reserved identifiers or producing a value).
  If it consumed the specified string exactly, parsing succeeds; otherwise, it fails.
Error::
  A `lexy::expected_keyword` error if it could not match the keyword.

CAUTION: Just because a `dsl::keyword()` rule was used somewhere, doesn't mean that the keyword is reserved.
For that, it has to be passed to `.reserve()`; it doesn't matter if it is given a keyword or a plain literal rule in that context.

NOTE: `keyword<Str>(id)` requires C++20 support for extended NTTPs. Use the `LEXY_KEYWORD(Str, id)` macro if your compiler does not support them.

[discrete]
==== `lexy::dsl::symbol()`

.`lexy/dsl/symbol.hpp`
[source,cpp]
----
namespace lexy
{
    template <typename T, typename... Strings>
    class symbol-table
    {
    public:
        using char_type   = /* ... */;
        using key_type    = char_type;
        using mapped_type = T;

        struct value_type
        {
            const char_type*   symbol;
            const mapped_type& value;
        };

        //=== modifiers ===//
        template <auto SymbolString, typename... Args>
        consteval symbol-table map(Args&&... args) const;

        template <auto C, typename... Args>
        consteval symbol-table map(Args&&... args) const;

        //=== access ===//
        static constexpr bool empty() noexcept;

        static constexpr std::size_t size() noexcept;

        class iterator;

        constexpr iterator begin() const noexcept;
        constexpr iterator end() const noexcept;

        class key_index
        {
        public:
            constexpr key_index() noexcept;
            constexpr explicit key_index(std::size_t idx) noexcept;

            constexpr explicit operator bool() const noexcept;

            friend constexpr bool operator==(key_index lhs, key_index rhs) noexcept;
            friend constexpr bool operator!=(key_index lhs, key_index rhs) noexcept;
        };

        template <typename Reader>
        constexpr key_index try_parse(Reader& reader) const;

        constexpr const T& operator[](key_index idx) const noexcept;
    };

    template <typename T>
    constexpr auto symbol_table = symbol-table<T>();
}
----

A symbol table is a fixed mapping of strings to some values of a type `T`.
It is created by calling `.map()` repeatedly on `lexy::symbol_table<T>`.
The table is then used by the `dsl::symbol()` rule.

[%collapsible]
.Example
====
[source,cpp]
----
// If C++20 NTTPs are supported:
constexpr auto table = lexy::symbol_table<int>.map<"abc">(0).map<'?'>(42);
// If they are not:
constexpr auto table = lexy::symbol_table<int>.map<LEXY_SYMBOl("abc")>(0).map<'?'>(42);
----
====

.`lexy/dsl/symbol.hpp`
----
symbol<table>(token) : Branch
symbol<table>(id)    : Branch = symbol<table>(id.pattern())
----

The `dsl::symbol()` rule matches `token` against one of the symbols given in the `table` and produces the corresponding value.

Branch Condition::
  Branch is only taken if `token` matches and the resulting lexeme is one of the symbols.
Matches::
  Matches and consumes `token`.
  Checks the resulting lexeme against all the symbols in the table, succeeds if that matches.
Errors::
  All errors raised by matching `token`.
  A generic error of tag `lexy::unknown_symbol` if the lexeme is not found in the symbol table.
  The tag can be overridden by calling `.error<Tag>`.
Value::
  Produces a single value, which is the value that corresponds to the lexeme according to the symbol table.

=== Delimited and quoted

.`lexy/dsl/delimited.hpp`
----
delimited(open_branch, close_branch)
delimited(open_branch, close_branch).open()  : Branch = open_branch
delimited(open_branch, close_branch).close() : Branch = close_branch
delimited(open_branch, close_branch).limit(token_1, ..., token_n)
----

A set of open and close delimiters can be specified using `delimited()`.
The result is not a rule, but a DSL for specifying a sequence of code points to be matched between the delimiters.
The open and close delimiters are defined using branches; they are returned by calling `.open()` and `.close()`.

An optional limit can be specified by calling `.limit()`.
If one of the tokens specified there is found in the input before the closing delimiter has been found,
it assumes that there is a missing closing delimiter and raises the appropriate error earlier.
This can be used to help error recovery by exiting early instead of consuming the entire input.

.`lexy/dsl/delimited.hpp`
----
delimited(branch) = delimited(branch, branch)
----

There is a convenience overload if the same rule is used for the open and closing delimiters.

.`lexy/dsl/delimited.hpp`
----
quoted        = delimited(lit<"\"">)
triple_quoted = delimited(lit<"\"\"\"">)

single_quoted = delimited(lit<"'">)

backticked        = delimited(lit<"`">)
double_backticked = delimited(lit<"``">)
triple_backticked = delimited(lit<"```">)
----

Common delimiters are predefined.

NOTE: The naming of `quoted`, `triple_quoted` and `single_quoted` is not very logical, but reflects common usage.

[discrete]
==== Simple delimited

.`lexy/dsl/delimited.hpp`
----
d(token) : Branch
----

Calling `d(token)`, where `d` is the result of a `delimiter()` call, results in a rule that matches `token` as often as possible surrounded by the delimiters.
Everything between the delimiters is captured and forwarded to a sink callback.

Requires::
  A production whose rule contains a delimited rule must provide a sink.
Branch Condition::
  Whatever the opening delimiter uses as branch condition.
Matching::
  Matches and consumes the opening delimiter, followed by zero or more occurrences of `token`, followed by the closing delimiter.
  It determines whether or not to parse another instance of `token` using the condition of the closing delimiter.
  Automatic whitespace skipping is disabled while matching the opening and closing delimiter, as well as `token` using `lexy::dsl::no_whitespace()`.
  After the closing delimiter has been matched and consumed, whitespace is skipped by matching and consuming `lexy::dsl::whitespace`.
Values::
  Values produced by the opening delimiter, the finished sink (which might be empty), and values produced by the closing delimiter.
  Everything captured by matching the `token` is forwarded to the sink.
Errors::
  All errors raised when matching the opening delimiter and the token.
  If EOF is reached without a closing delimiter, a generic error with tag `lexy::missing_delimiter` is raised.

[%collapsible]
.Example
====
[source,cpp,id=nnoMYv]
----
// Match a string consisting of code points that aren't control characters.
dsl::quoted(dsl::code_point - dsl::ascii::control)
----
====

NOTE: The sink is only invoked once. A sink callback is only required for consistency with the overload that takes an escape sequence.

[discrete]
==== Delimited with escape sequences

.`lexy/dsl/delimited.hpp`
----
d(token, escape)  : Branch
----

This overload is used to to specify escape sequences in the delimited.
It behaves like the other overload, but also matches the `escape` rule.

Requires::
  A production whose rule contains a delimited rule must provide a sink.
  `escape` must be a branch.
Branch Condition::
  Whatever the opening delimiter uses as branch condition.
Matching::
  Matches and consumes the opening delimiter, followed by zero or more occurrences of `escape` or `token`, followed by the closing delimiter.
  It determines whether or not to parse another instance of `token` using the condition of the closing delimiter.
  It first tries to match `escape`, and only then `token`.
  .
  Automatic whitespace skipping is disabled while matching the opening and closing delimiter, as well as `token` using `lexy::dsl::no_whitespace()`.
  After the closing delimiter has been matched and consumed, whitespace is skipped by matching and consuming `lexy::dsl::whitespace`.
Values::
  Values produced by the opening delimiter, the finished sink (which might be empty), and values produced by the closing delimiter.
  Everything captured by matching the `token` is forwarded to the sink, as well as all values produced by `escape`.
Errors::
  All errors raised when matching the opening delimiter, `escape` and the token.
  If EOF is reached without a closing delimiter, a generic error with tag `lexy::missing_delimiter` is raised.

[%collapsible]
.Example
====
[source,cpp,id=vqsfM4]
----
// Match a string consisting of code points that aren't control characters.
// `\"` can be used to add a `"` to the string.
dsl::quoted(dsl::code_point - dsl::ascii::control,
            LEXY_LIT("\\\"") >> dsl::value_c<'"'>)
----
====

NOTE: The closing delimiter is used as termination condition here as well.
If the escape sequence starts with a closing delimiter, it will not be matched.

[discrete]
==== `lexy::dsl::escape()`

.`lexy/dsl/delimited.hpp`
----
escape(token) : Rule
----

For convenience, the `escape` rule can be used to specify the escape token.

An escape rule consists of a leading token that matches the escape character (e.g. `\`), and zero or more alternatives for characters that can be escaped.
It then is equivalent to `token >> (alt0 | alt1 | alt2 | error<lexy::invalid_escape_sequence>)`.
It will only be considered after the leading token has been matched and then tries to match one of the alternatives.
If no alternative matches, it raises a generic error with tag `lexy::invalid_escape_sequence`.

.`lexy/dsl/delimited.hpp`
----
e.rule(branch) : Rule
  = escape_token >> ( ... | branch
                      | else_ >> error<lexy::invalid_escape_sequence>)
----

Calling `e.rule(branch)`, where `e` is an escape rule, adds `branch` to the end of the choice.

.`lexy/dsl/delimited.hpp`
----
e.capture(token) : Rule
  = escape_token >> (... | capture(token)
                      | else_ >> error<lexy::invalid_escape_sequence>)
----

Calling `e.capture(token)`, where `e` is an escape rule, adds an escape sequence that matches and captures token to the end of the choice.

.`lexy/dsl/delimited.hpp`
----
e.lit<Str>(rule) : Rule
  = escape_token >> (... | lit<Str> >> rule
                      | else_ >> error<lexy::invalid_escape_sequence>)
e.lit<Str>() : Rule
  = e.lit<Str>(value_str<Str>)

e.lit_c<C>(rule) : Rule
  = escape_token >> (... | lit_c<C> >> rule
                      | else_ >> error<lexy::invalid_escape_sequence>)
e.lit_c<C>() : Rule
  = e.lit_c<C>(value_c<C>)
----

Calling `e.lit()` or `e.lit_c()`, where `e` is an escape rule, adds an escape sequences that matches the literal and produces the values of the rule to the end of the choice.
If no rule is specified, it defaults to producing the literal itself.

.`lexy/dsl/delimited.hpp`
----
e.symbol<Table>(rule) : Rule
  = escape_token >> (... | symbol<Table>(rule)
                     | else_ >> error<lexy::invalid_escape_sequence>)
e.symbol<Table>() : Rule
  = escape_token >> (... | symbol<Table>(single-code-unit)
                     | else_ >> error<lexy::invalid_escape_sequence>)
----

Calling `e.symbol<Table>(rule)`, where `e` is an escape rule, adds an escape sequence that matches one of the symbols in the `Table` to the end of the choice.
The overload that does not take a `rule`, matches a single code unit that is then looked up in the table.

NOTE: Use this overload to specify common escape sequences such as `\n`, `\"` etc. by creating a symbol table that matches the escape sequence character (e.g. `n`) to the character it stands for (e.g. `\n`).

.`lexy/dsl/delimited.hpp`
----
backslash_escape = escape(lit_c<'\\'>)
dollar_escape    = escape(lit_c<'$'>)
----

Common escape characters are predefined.

[%collapsible]
.Example
====
[source,cpp,id=71EEWY]
----
// Match a string consisting of code points that aren't control characters.
// `\"` can be used to add a `"` to the string.
// `\uXXXX` can be used to add the code point with the specified value.
dsl::quoted(dsl::code_point - dsl::ascii::control,
            dsl::backslash_escape
              .lit_c<'"'>()
              .rule(dsl::lit_c<'u'> >> dsl::code_point_id<4>)
----
====

=== Aggregates

.`lexy/dsl/member.hpp`
----
member<MemPtr> = rule   : Rule
member<MemPtr> = branch : Branch

LEXY_MEM(Name) = rule   : Rule
LEXY_MEM(Name) = branch : Branch
----

The `member` rule together with the `lexy::as_aggregate<T>` callback assigns the values produced by the rule given to it via `=` to the specified member of the aggregate `T`.

Requires::
  A production that contains a member rule needs to use `lexy::as_aggregate<T>` as sink or callback.
  The rule must produce exactly one value.
Matches::
  Matches and consumes the `rule` given to it via `=`.
Values::
  Produces two values.
  The first value identifiers the targeted member.
  For `member<MemPtr>`, this is the member pointed to by the member pointer.
  For `LEXY_MEM(Name)`, it is the member with the given `Name`.
  The second value is the value produced by the rule.
Errors::
  All errors raised during parsing of the assigned rule.

The `lexy::as_aggregate<T>` callback, collects all member and value pairs.
It then constructs an object of type `T` using value initialization and for each pair assigns the value to the specified member of it.
This works either as callback or a sink.
If a member is specified more than once, the final value is stored at the end.

[%collapsible]
.Example
====
[source,cpp,id=EMYGx1]
----
// Parses two integers separated by commas.
// The first integer is assigned to a member called `second`,
// the second integer is assigned to a member called `first`.
(LEXY_MEM(second) = dsl::integer<int>(dsl::digits<>))
+ dsl::comma
+ (LEXY_MEM(first) = dsl::integer<int>(dsl::digits<>))
----
====

=== Context sensitive parsing

To parse context sensitive grammars, `lexy` allows the creation of _context variables_.
They allow to save state between different rules which can be used to parse context sensitive elements such as XML with matching opening and closing tag names.

A context variable has a type, which is limited to `bool`, `int` and `lexy::lexeme`, and an identifier, which is given by a type.
Before a variable can be used it needs to be created with `.create()`.
It is then available for all rules of the current production: child and parent production cannot access them.
Variables are not persistent between multiple invocations of a production;
every time a production is parsed it starts out with no variables.

See `example/xml.cpp` for an example that uses the context sensitive parsing facilities.

[discrete]
==== `lexy::dsl::context_flag`

.`lexy/dsl/context_flag.hpp`
----
context_flag<Id>
----

A `lexy::dsl::context_flag` controls a boolean that can be `true` or `false`.
Each object is uniquely identified by the type `Id`.
It is not a rule but a DSL for specifying operations which are then rules.

----
context_flag<Id>.create() : Rule
context_flag<Id>.create<Value>() : Rule
----

The `.create()` rule does not interact with the input at all.
When it is parsed, it creates the flag with the given `Id` and initializes it to the `Value` (defaulting to `false`).

----
context_flag<Id>.set()   : Rule
context_flag<Id>.reset() : Rule
----

The `.set()`/`.reset()` rules do not interact with the input at all.
When they are parsed, they set the flag with the given `Id` to `true`/`false` respectively.

----
context_flag<Id>.toggle() : Rule
----

The `.toggle()` rule does not interact with the input at all.
When it is parsed, it toggles the value of the flag with the given `Id`.

----
context_counter<Id>.select(rule_true, rule_false) : Rule
----

The `.select()` rule selects on the given rules depending on the value of the flag with the given `Id`.
It then parses the selected rule.

Matches::
  If the value of the flag is `true`, matches and  consumes `rule_true`.
  Otherwise, matches and  consumes `rule_false`.
Values::
  All values produced by parsing the selected rule.
Errors::
  All errors raised by parsing the selected rule.

----
context_flag<Id>.require().error<Tag>        : Rule
context_flag<Id>.require<Value>().error<Tag> : Rule
----

The `.require()` rule does not interact with the input at all.
When it is parsed, it checks that the value of the flag with the given `Id` is the given `Value` (defaults to `true`).
If that is the case, parsing continues.
Otherwise, the rule fails, producing an error with the given `Tag`.

[discrete]
==== `lexy::dsl::context_counter`

.`lexy/dsl/context_counter.hpp`
----
context_counter<Id>
----

A `lexy::dsl::context_counter` controls a C++ `int`.
Each object is uniquely identified by the type `Id`.
It is not a rule but a DSL for specifying operations which are then rules.

----
context_counter<Id>.create() : Rule
context_counter<Id>.create<Value>() : Rule
----

The `.create()` rule does not interact with the input at all.
When it is parsed, it creates the counter with the given `Id` and initializes it to the `Value` (defaulting to `0`).

----
context_counter<Id>.inc() : Rule
context_counter<Id>.dec() : Rule
----

The `.inc()`/`.dec()` rules do not interact with the input at all.
When they are parsed, they increment/decrement the counter with the given `Id` respectively.

----
context_counter<Id>.push(rule) : Rule
context_counter<Id>.pop(rule)  : Rule
----

The `.push()`/`.pop()` rules parse the given `rule`.
The counter with the given `Id` is then incremented/decremented by the number of characters (code units) consumed by `rule`.

Matches::
  Matches and  consumes `rule`.
Values::
  All values produced by parsing `rule`.
Errors::
  All errors raised by parsing `rule`.

----
context_counter<Id>.compare<Value>(rule_less, rule_eq, rule_greater) : Rule
----

The `.compare()` rule compares the value of the counter with the given `Id` to `Value`.
It then parses one of the three given rules, depending on the result.

Matches::
  If the value of the counter is less than `Value`, matches and  consumes `rule_less`.
  If the value of the counter is equal than `Value`, matches and  consumes `rule_eq`.
  If the value of the counter is greater than `Value`, matches and  consumes `rule_greater`.
Values::
  All values produced by parsing the selected rule.
Errors::
  All errors raised by parsing the selected rule.

----
context_counter<Id>.require().error<Tag>        : Rule
context_counter<Id>.require<Value>().error<Tag> : Rule
----

The `.require()` rule does not interact with the input at all.
When it is parsed, it checks that the value of the counter with the given `Id` is the given `Value` (defaults to `0`).
If that is the case, parsing continues.
Otherwise, the rule fails, producing an error with the given `Tag`.

[discrete]
==== `lexy::dsl::context_lexeme`

.`lexy/dsl/context_lexeme.hpp`
----
context_lexeme<Id>
----

A `lexy::dsl::context_flag` controls a `lexy::lexeme` (i.e. a string view on part of the input).
Each object is uniquely identified by the type `Id`.
It is not a rule but a DSL for specifying operations which are then rules.

----
context_lexeme<Id>.create() : Rule
----

The `.create()` rule does not interact with the input at all.
When it is parsed, it creates the lexeme with the given `Id` and initializes it to an empty view.

----
context_lexeme<Id>.capture(rule) : Rule
----

The `.capture()` rule parses the given `rule`.
The lexeme with the given `Id` is then set to view everything the `rule` has consumed as-if `lexy::dsl::capture()` was used.

Matches::
  Matches and  consumes `rule.`
Values::
  All values produced by parsing `rule`.
Errors::
  All errors raised by parsing `rule`.

----
context_lexeme<Id>.require(rule).error<Tag> : Rule
----

The `.require()` rule parses the given `rule`, capturing it in a temporary lexeme.
The temporary lexeme is then compared with the lexeme given by the `Id`.
If the two lexemes are equal, parsing continues.
Otherwise, the rule fails, producing an error with the given `Tag`.

Matches::
  Matches and  consumes `rule.`
Values::
  Discards values produced by `rule`.
Errors::
  All errors raised by parsing `rule`.
  A generic error with the given `Tag` if the rule did not match.

=== Raw input

The following facilities are meant for parsing input that uses the `lexy::byte_encoding`, that is input consisting of bytes, not text.

[discrete]
==== `lexy::dsl::bom`

.`lexy/dsl/bom.hpp`
----
bom<Encoding, Endianness> : Token
----

The `bom` token matches the byte-order mark (BOM) for the given encoding and `lexy::encoding_endianness`.

Requires::
  `Endianness` is `lexy::encoding_endianness::little` or `lexy::encoding_endianness::big`.
Matches::
  If the encoding has a BOM, matches and consumes the BOM written in the given endianness.
Errors::
  A `lexy::expected_char_class` error with the name `BOM.<encoding>-<endianness>` if the BOM was not matched.

[%collapsible]
.Example
====
[source,cpp,id=xbnEYs]
----
// Matches the UTF-16 big endian BOM (0xFE, 0xFF).
dsl::bom<lexy::utf16_encoding, lexy::encoding_endianness::big>
----
====

NOTE: There is a UTF-8 BOM, but it is the same regardless of endianness.

NOTE: This rule is only necessary when you have a raw encoding that contains a BOM.
For example, `lexy::read_file()` already handles and deals with BOMs for you by default.

[discrete]
==== `lexy::dsl::encode`

.`lexy/dsl/encode.hpp`
----
encode<Encoding, Endianness>(rule) : Rule
----

The `encode` rule temporarily changes the encoding of the input.
The specified `rule` will be matched using a `Reader` whose encoding is `Encoding` converted from the raw bytes using the specified endianness.
If no `Endianness` is specified, the default is `lexy::encoding_endianness::bom`, and a BOM is matched on the input to determine the endianness.
If no BOM is present, big endian is assumed.

Requires::
  The input's encoding is a single-byte encoding (usually `lexy::byte_encoding`).
Matches::
  If the endianness is `lexy::encoding_endianness::bom`, matches and consumes an optional BOM to determine endianness.
  Matches and consumes `rule`.
  However, the input of rule are characters according to `Encoding` and `Endianness`, not the single bytes of the actual input.
Values::
  All values produced by the rule.
Errors::
  All errors raised by the rule.
  The error type uses the original reader, not the encoded reader that does the input translation.

[%collapsible]
.Example
====
[source,cpp,id=Y51r9v]
----
// Matches a UTF-8 code point, followed by an ASCII code point.
dsl::encode<lexy::utf8_encoding>(dsl::code_point)
    + dsl::encode<lexy::ascii_encoding>(dsl::code_point)
----
====

=== Custom rules

The exact interface for the `Rule`, `Token` and `Branch` concepts is currently still experimental.
Refer to the existing rules if you want to add your own.

